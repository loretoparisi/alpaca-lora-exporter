{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "n72bGvc-7zeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3qbyAHt7nkY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel, LoraConfig\n",
        "\n",
        "import transformers\n",
        "\n",
        "from transformers.models.llama.tokenization_llama import LLaMATokenizer\n",
        "from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM \n",
        "\n",
        "BASE_MODEL = None\n",
        "LORA_MODEL = None\n",
        "\n",
        "assert (\n",
        "    BASE_MODEL\n",
        "), \"Please specify a BASE_MODEL in the script, e.g. 'decapoda-research/llama-7b-hf'\"\n",
        "\n",
        "assert (\n",
        "    LORA_MODEL\n",
        "), \"Please specify a BASE_MODEL in the script, e.g. 'tloen/alpaca-lora-7b'\"\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "base_model = LLaMAForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_8bit=False,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": \"cpu\"},\n",
        ")\n",
        "\n",
        "lora_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    LORA_MODEL,\n",
        "    device_map={\"\": \"cpu\"},\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# merge weights\n",
        "for layer in lora_model.base_model.model.model.layers:\n",
        "    layer.self_attn.q_proj.merge_weights = True\n",
        "    layer.self_attn.v_proj.merge_weights = True\n",
        "\n",
        "lora_model.train(False)\n",
        "\n",
        "lora_model_sd = lora_model.state_dict()\n",
        "\n",
        "params = {\n",
        "    \"dim\": 4096,\n",
        "    \"multiple_of\": 256,\n",
        "    \"n_heads\": 32,\n",
        "    \"n_layers\": 32,\n",
        "    \"norm_eps\": 1e-06,\n",
        "    \"vocab_size\": -1,\n",
        "}\n",
        "n_layers = params[\"n_layers\"]\n",
        "n_heads = params[\"n_heads\"]\n",
        "dim = params[\"dim\"]\n",
        "dims_per_head = dim // n_heads\n",
        "base = 10000.0\n",
        "inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n",
        "\n",
        "\n",
        "def permute(w):\n",
        "    return (\n",
        "        w.view(n_heads, dim // n_heads // 2, 2, dim).transpose(1, 2).reshape(dim, dim)\n",
        "    )\n",
        "\n",
        "\n",
        "def unpermute(w):\n",
        "    return (\n",
        "        w.view(n_heads, 2, dim // n_heads // 2, dim).transpose(1, 2).reshape(dim, dim)\n",
        "    )\n",
        "\n",
        "\n",
        "def translate_state_dict_key(k):\n",
        "    k = k.replace(\"base_model.model.\", \"\")\n",
        "    if k == \"model.embed_tokens.weight\":\n",
        "        return \"tok_embeddings.weight\"\n",
        "    elif k == \"model.norm.weight\":\n",
        "        return \"norm.weight\"\n",
        "    elif k == \"lm_head.weight\":\n",
        "        return \"output.weight\"\n",
        "    elif k.startswith(\"model.layers.\"):\n",
        "        layer = k.split(\".\")[2]\n",
        "        if k.endswith(\".self_attn.q_proj.weight\"):\n",
        "            return f\"layers.{layer}.attention.wq.weight\"\n",
        "        elif k.endswith(\".self_attn.k_proj.weight\"):\n",
        "            return f\"layers.{layer}.attention.wk.weight\"\n",
        "        elif k.endswith(\".self_attn.v_proj.weight\"):\n",
        "            return f\"layers.{layer}.attention.wv.weight\"\n",
        "        elif k.endswith(\".self_attn.o_proj.weight\"):\n",
        "            return f\"layers.{layer}.attention.wo.weight\"\n",
        "        elif k.endswith(\".mlp.gate_proj.weight\"):\n",
        "            return f\"layers.{layer}.feed_forward.w1.weight\"\n",
        "        elif k.endswith(\".mlp.down_proj.weight\"):\n",
        "            return f\"layers.{layer}.feed_forward.w2.weight\"\n",
        "        elif k.endswith(\".mlp.up_proj.weight\"):\n",
        "            return f\"layers.{layer}.feed_forward.w3.weight\"\n",
        "        elif k.endswith(\".input_layernorm.weight\"):\n",
        "            return f\"layers.{layer}.attention_norm.weight\"\n",
        "        elif k.endswith(\".post_attention_layernorm.weight\"):\n",
        "            return f\"layers.{layer}.ffn_norm.weight\"\n",
        "        elif k.endswith(\"rotary_emb.inv_freq\") or \"lora\" in k:\n",
        "            return None\n",
        "        else:\n",
        "            print(layer, k)\n",
        "            raise NotImplementedError\n",
        "    else:\n",
        "        print(k)\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "new_state_dict = {}\n",
        "for k, v in lora_model_sd.items():\n",
        "    new_k = translate_state_dict_key(k)\n",
        "    if new_k is not None:\n",
        "        if \"wq\" in new_k or \"wk\" in new_k:\n",
        "            new_state_dict[new_k] = unpermute(v)\n",
        "        else:\n",
        "            new_state_dict[new_k] = v\n",
        "\n",
        "os.makedirs(\"./ckpt\", exist_ok=True)\n",
        "\n",
        "torch.save(new_state_dict, \"./ckpt/consolidated.00.pth\")\n",
        "\n",
        "with open(\"./ckpt/params.json\", \"w\") as f:\n",
        "    json.dump(params, f)"
      ]
    }
  ]
}